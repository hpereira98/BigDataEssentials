1. Estimate minimum Namenode RAM size for HDFS with 1 PB capacity, block size 64 MB, average metadata size for each block is 300 B, replication factor is 3. Provide the formula for calculations and the result.
Namenode RAM Size = DFS Capacity / (Block Size * Replication Factor) * Average Namenode Block Size
Namenode RAM Size = 1PB / (64 MB * 3) * 300
Namenode RAM Size = 1000000000000000 / ( 64 000 000  * 3 ) * 300
Namenode RAM Size = 1562500000B = 1.5625GB

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. HDDs in your cluster have the following characteristics: average reading speed is 60 MB/s, seek time is 5 ms. You want to spend 0.5 % time for seeking the block, i.e. seek time should be 200 times less than the time to read the block. Estimate the minimum block size.

Average Reading Speed = 60MB/s = 60 000 000 B/s
Seek Time = 5ms = 0.005s
0.5% = 0.005
Minimum Block Size / Average Reading Speed *  Time for Seeking the Block >= Seek Time
X / 60 000 000 * 0.005 >= 0.005
X = 60 000 000 B = 60MB

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Create text file ‘test.txt’ in a local fs. Use HDFS CLI to make the following operations:

сreate directory ‘assignment1’ in your home directory in HDFS (you can use a relative path or prescribe it explicitly "/user/jovyan/...")
put test.txt in it
output the size and the owner of the file
revoke ‘read’ permission for ‘other users’
read the first 10 lines of the file
rename it to ‘test2.txt’.
Provide all the commands to HDFS CLI.

hdfs dfs -mkdir -p assignment1
hdfs dfs -put test.txt assignment1
hdfs dfs -ls assignment1/test.txt
hdfs dfs -chmod o-r assignment1/test.txt
hdfs dfs -cat assignment1/test.txt | head -10
hdfs dfs -mv assignment1/test.txt assignment1/test2.txt

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Use HDFS CLI to investigate the file ‘/data/wiki/en_articles_part/articles-part’ in HDFS:

get blocks and their locations in HDFS for this file, show the command without an output
get the information about any block of the file, show the command and the block locations from the output

hdfs fsck /data/wiki/en_articles_part/articles-part -files -blocks -locations
hdfs fsck -blockId blk_1073741828

Connecting to namenode via http://localhost:50070/fsck?ugi=jovyan&blockId=blk_1073741828+&path=%2F
 FSCK started by jovyan (auth:SIMPLE) from /127.0.0.1 at Fri Sep 18 01:48:56 GMT 2020
 
 Block Id: blk_1073741828
 Block belongs to: /data/wiki/en_articles_part/articles-part
 No. of Expected Replica: 1
 No. of live Replica: 1
 No. of excess Replica: 0
 No. of stale Replica: 0
 No. of decommissioned Replica: 0
 No. of decommissioning Replica: 0
 No. of corrupted Replica: 0
 Block replica on datanode/rack: 172.17.0.46/default-rack is HEALTHY

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Look at the picture of Namenode web interface from a real Hadoop cluster. Show the total capacity of this HDFS installation, used space and total data nodes in the cluster.

Total Capacity: 2.14TB
Used Space: 242.12GB + 35.51GB = 277,63GB
Total Data Nodes: 4
1. Estimate minimum Namenode RAM size for HDFS with 1 PB capacity, block size 64 MB, average metadata size for each block is 300 B, replication factor is 3. Provide the formula for calculations and the result.
Namenode RAM Size = DFS Capacity / (Block Size * Replication Factor) * Average Namenode Block Size
Namenode RAM Size = 1PB / (64 MB * 3) * 300
Namenode RAM Size = 1000000000000000 / ( 64 000 000  * 3 ) * 300
Namenode RAM Size = 1562500000B = 1.5625GB

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. HDDs in your cluster have the following characteristics: average reading speed is 60 MB/s, seek time is 5 ms. You want to spend 0.5 % time for seeking the block, i.e. seek time should be 200 times less than the time to read the block. Estimate the minimum block size.

Average Reading Speed = 60MB/s = 60 000 000 B/s
Seek Time = 5ms = 0.005s
0.5% = 0.005
Minimum Block Size = Average Reading Speed * Seek Time * 0.5%
0.005 * 60 000 000 * 0.005 = 1500B

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Create text file ‘test.txt’ in a local fs. Use HDFS CLI to make the following operations:

сreate directory ‘assignment1’ in your home directory in HDFS (you can use a relative path or prescribe it explicitly "/user/jovyan/...")
put test.txt in it
output the size and the owner of the file
revoke ‘read’ permission for ‘other users’
read the first 10 lines of the file
rename it to ‘test2.txt’.
Provide all the commands to HDFS CLI.

hdfs dfs -mkdir -p /user/jovyan/assignment1
hdfs dfs -put test.txt /user/jovyan/assignment1
hdfs dfs -ls -h /user/jovyan/assignment1/test.txt
hdfs dfs -chmod o-r /user/jovyan/assignment1/test.txt
hdfs dfs -cat /user/jovyan/assignment1/test.txt | head -10
hdfs dfs -mv /user/jovyan/assignment1/test.txt /user/jovyan/assignment1/test2.txt

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Use HDFS CLI to investigate the file ‘/data/wiki/en_articles_part/articles-part’ in HDFS:

get blocks and their locations in HDFS for this file, show the command without an output
get the information about any block of the file, show the command and the block locations from the output
hdfs fsck /data/wiki/en_articles_part/articles-part -blocks -locations
hdfs fsck -blockId XXXXX ----> here XXXXX is one of the block ids gotten from the above command's output (I am not being able to access the HDFS CLI Playground)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Look at the picture of Namenode web interface from a real Hadoop cluster. Show the total capacity of this HDFS installation, used space and total data nodes in the cluster.

Total Capacity: 2.14TB
Used Space: 242.12GB (11.03%)
Total Data Nodes: 4